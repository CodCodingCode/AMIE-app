# Complete Installation Guide for GH200 Training Setup

# 1) System Prerequisites
# Verify your system (you already have these):
nvidia-smi  # Should show NVIDIA driver â‰¥ 535 (you have 570.124.06)
nvcc --version  # Should show CUDA 12.8

# 2) Create & Activate Python Virtual Environment
cd ~/project
python3 -m venv llama3-ft
source llama3-ft/bin/activate
pip install --upgrade pip setuptools wheel

# 3) Install PyTorch with CUDA 12.8 Support
pip install torch torchvision torchaudio \
    --index-url https://download.pytorch.org/whl/cu128

# 4) Install Specific Transformers Version (4.45 - the working version)
# Install the working transformers version first
pip install transformers==4.45.0

# Install other Hugging Face dependencies
pip install \
    datasets \
    accelerate \
    tokenizers \
    safetensors \
    sentencepiece \
    huggingface_hub

# 5) Install TRL (for GRPO training)
# Install TRL for GRPO functionality
pip install trl

# Or if you need the latest features:
# pip install git+https://github.com/huggingface/trl.git

# 6) Fix TF-Keras Import Issues
# Option A: Install backwards-compatible tf-keras
pip install tf-keras

# Option B: If you want minimal TensorFlow footprint
# pip install tensorflow-cpu --no-deps

# 7) Optional Performance Packages
# For better performance (optional)
pip install \
    flash-attn \
    bitsandbytes \
    peft

# 8) Set Environment Variables
# Add to your ~/.bashrc or run before training
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8
export HF_HOME=~/.cache/huggingface
export TRANSFORMERS_CACHE=~/.cache/huggingface
export TOKENIZERS_PARALLELISM=false  # Avoid warnings
export CUDA_VISIBLE_DEVICES=0

# 9) Verification Tests
# Test CUDA and PyTorch
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}')"
python -c "import torch; print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"

# Test transformers version
python -c "import transformers; print(f'Transformers version: {transformers.__version__}')"

# Test TRL
python -c "import trl; print(f'TRL version: {trl.__version__}')"

# Test datasets
python -c "import datasets; print(f'Datasets version: {datasets.__version__}')"

# 10) Final Package Versions Check
pip list | grep -E "(torch|transformers|trl|datasets|accelerate)"

# Expected output should show:
# - torch ~2.4.0+cu128
# - transformers 4.45.0
# - trl latest version
# - datasets latest version
# - accelerate latest version

# 11) Ready to Train!
# Your environment is now set up with the working transformers version. 
# You can now run your training script with multiprocessing enabled:
# dataloader_num_workers=4,
# dataloader_persistent_workers=True,

# TROUBLESHOOTING

# If you get import errors:
# pip uninstall torch transformers trl -y
# pip install torch --index-url https://download.pytorch.org/whl/cu128
# pip install transformers==4.45.0
# pip install trl

# If you get CUDA out of memory:
# - Reduce per_device_train_batch_size from 4 to 2 or 3
# - Increase gradient_accumulation_steps accordingly

# If you get worker process errors:
# - Verify transformers version: python -c "import transformers; print(transformers.__version__)"
# - Should be exactly 4.45.0